@inproceedings{icml22,
  abbr      = {ICML},
  title     = {Improving Mini-batch Optimal Transport via Partial Transportation},
  author    = {Khai Nguyen* and Dang Nguyen* and The-Anh Vu-Le and Tung Pham and Nhat Ho},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  year      = {2022},
  month     = {Jul},
  pdf       = {https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf},
  code      = {https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT},
  abstract  = {Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mappings, namely, mappings that are optimal on the mini-batch level but are partially wrong in the comparison with the optimal transportation plan between the original measures. Motivated by the misspecified mappings issue, we propose a novel mini-batch method by using partial optimal transport (POT) between mini-batch empirical measures, which we refer to as mini-batch partial optimal transport (m-POT). Leveraging the insight from the partial transportation, we explain the source of misspecified mappings from the m-OT and motivate why limiting the amount of transported masses among mini-batches via POT can alleviate the incorrect mappings. Finally, we carry out extensive experiments on various applications such as deep domain adaptation, partial domain adaptation, deep generative model, color transfer, and gradient flow to demonstrate the favorable performance of m-POT compared to current mini-batch methods.}
}

@inproceedings{acmmm23,
  abbr      = {ACMMM},
  title     = {Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment},
  author    = {The-Anh Vu-Le* and Cong-Duy Nguyen* and Thong Nguyen and Tho Quan and Anh-Tuan Luu},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  year      = {2023},
  month     = {Oct},
  pdf       = {https://dl.acm.org/doi/pdf/10.1145/3581783.3612248},
  abstract  = {Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically its partial variant, to solve the fractional alignment problem between the two modalities. Our proposed method significantly outperforms the baseline language models on various language tasks of the GLUE and SQuAD datasets.}
}
